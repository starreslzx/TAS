import json
from typing import Dict, List, Tuple, Optional, Any
import re
from collections import defaultdict
import hashlib
import uuid
import time
import threading
from datetime import datetime
import numpy as np
import os

# å¯¼å…¥æ ¸å¿ƒè®¡ç®—åº“
import jieba
import jieba.analyse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

try:
    from openai import OpenAI

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAIåº“æœªå®‰è£…ï¼ŒAPIåŠŸèƒ½å°†ä¸å¯ç”¨")

from collections import Counter


class TopicGraph:
    """
    è¯é¢˜å›¾ç®¡ç†å™¨ - ä¿®å¤åˆå¹¶é—®é¢˜ç‰ˆæœ¬
    """

    def __init__(self, json_file: str = None,
                 auto_cleanup_days: int = 30,
                 similarity_threshold: float = 0.3,
                 enable_api: bool = False,
                 api_key: str = "",
                 base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                 debug_mode: bool = True):

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if json_file:
            output_dir = os.path.dirname(json_file)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
                if debug_mode:
                    print(f"ğŸ“ åˆ›å»ºç›®å½•: {output_dir}")

        self.chat_groups: List[Dict[str, Any]] = []
        self.json_file = json_file
        self.similarity_threshold = similarity_threshold
        self.enable_api = enable_api
        self.api_key = api_key
        self.base_url = base_url
        self.auto_cleanup_days = auto_cleanup_days
        self.debug_mode = debug_mode

        # åˆå§‹åŒ–å›¾ç»“æ„ç›¸å…³å±æ€§
        self.graph = {}
        self.topic_id_to_name = {}
        self.topic_name_to_id = {}
        self.parent_child_map = {}
        self.child_parent_map = {}
        self.topic_id_to_type = {}
        self.topic_embeddings = {}

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = None
        if self.enable_api and OPENAI_AVAILABLE and self.api_key:
            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            except Exception as e:
                print(f"âš ï¸ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None

        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            tokenizer=self._jieba_tokenizer,
            min_df=1,
            max_df=0.8,
            use_idf=True,
            smooth_idf=True
        )
        self._fit_vectorizer_vocab()

        # è‡ªåŠ¨æ¸…ç†çº¿ç¨‹ç›¸å…³
        self.running = False
        self.cleanup_thread = None

        # è°ƒè¯•ä¿¡æ¯å­˜å‚¨
        self.debug_logs = []
        self.similarity_calculations = []

        if json_file:
            success = self.load_from_json(json_file)
            if success:
                self._debug_print(f"âœ… ä»{json_file}åŠ è½½äº†{len(self.chat_groups)}ä¸ªç¾¤èŠ")
                self._debug_current_structure()
            else:
                self._debug_print(f"âš ï¸ æ— æ³•ä»{json_file}åŠ è½½æ•°æ®ï¼Œå°†ä½¿ç”¨ç©ºç»“æ„")

    def _debug_print(self, message: str, level: str = "INFO"):
        """è°ƒè¯•è¾“å‡º"""
        if self.debug_mode:
            timestamp = datetime.now().strftime("%H:%M:%S")
            formatted_message = f"[{timestamp}] [{level}] {message}"
            print(formatted_message)
            self.debug_logs.append(formatted_message)

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """ä½¿ç”¨è€ƒè™‘è¯é¢‘çš„Jaccardç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0

        words1 = list(jieba.cut(text1))
        words2 = list(jieba.cut(text2))

        if not words1 or not words2:
            return 0.0

        counter1 = Counter(words1)
        counter2 = Counter(words2)

        intersection = sum((counter1 & counter2).values())
        union = sum((counter1 | counter2).values())

        return intersection / union if union > 0 else 0.0

    def add_topic_simple(self, group_id: str, topic_name: str, priority: str,
                         description: str = "", related_topics: List[str] = None) -> Tuple[bool, str]:
        """
        æ·»åŠ è¯é¢˜ï¼ˆè‡ªåŠ¨è®¡ç®—ç›¸ä¼¼åº¦å¹¶åˆå¹¶ï¼‰- ä¿®å¤åˆå¹¶é€»è¾‘

        ä¸»è¦ä¿®å¤ç‚¹ï¼š
        1. ç¡®ä¿æ‰¾åˆ°ç›¸ä¼¼è¯é¢˜åæ‰§è¡Œåˆå¹¶
        2. æ­£ç¡®å¤„ç†åˆå¹¶å†³ç­–
        """
        if related_topics is None:
            related_topics = []

        self._debug_print(f"ğŸš€ å¼€å§‹æ·»åŠ è¯é¢˜: {topic_name}", "TOPIC_ADD")

        # æŸ¥æ‰¾ç¾¤ç»„
        group = None
        for g in self.chat_groups:
            if g['group_id'] == group_id:
                group = g
                break

        if not group:
            self._debug_print(f"âŒ ç¾¤ç»„ {group_id} ä¸å­˜åœ¨", "ERROR")
            return False, f"ç¾¤ç»„ {group_id} ä¸å­˜åœ¨"

        # æ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨é‡å¤çš„è¯é¢˜
        for topic in group.get('topics', []):
            if topic['topic_name'] == topic_name:
                self._debug_print(f"âŒ è¯é¢˜ '{topic_name}' å·²å­˜åœ¨", "ERROR")
                return False, f"è¯é¢˜ '{topic_name}' å·²å­˜åœ¨"

        # åˆ›å»ºæ–°è¯é¢˜å¯¹è±¡
        new_topic = {
            "topic_id": f"topic_{group_id.replace('group_', '')}_{len(group['topics']) + 1:04d}",
            "topic_name": topic_name,
            "priority": priority,
            "summaries": [description] if description else [],
            "related_records": [],
            "related_topics": related_topics,
            "is_major": False,
            "parent_id": None
        }

        # é¦–å…ˆæ·»åŠ æ–°è¯é¢˜åˆ°ç¾¤ç»„
        group['topics'].append(new_topic)
        self._debug_print(f"ğŸ“ æ–°è¯é¢˜å·²æ·»åŠ åˆ°ç¾¤ç»„", "TOPIC_ADD")

        # ç¬¬ä¸€æ­¥ï¼šæŸ¥æ‰¾ç›¸ä¼¼è¯é¢˜å¹¶è®°å½•
        self._debug_print(f"ğŸ” å¼€å§‹ç›¸ä¼¼åº¦æ‰«æ (é˜ˆå€¼={self.similarity_threshold})", "SIMILARITY_SCAN")

        # è¾“å‡ºç°æœ‰è¯é¢˜åˆ—è¡¨ï¼ˆæ’é™¤æ–°è¯é¢˜è‡ªå·±ï¼‰
        existing_topics = [t for t in group.get('topics', [])
                           if t['topic_id'] != new_topic['topic_id'] and not t.get('parent_id')]

        self._debug_print(f"  ç°æœ‰ç‹¬ç«‹è¯é¢˜æ•°: {len(existing_topics)}", "SIMILARITY_SCAN")
        for i, t in enumerate(existing_topics, 1):
            self._debug_print(f"    {i}. {t['topic_name']}", "SIMILARITY_SCAN")

        similar_topic_ids = []
        similarity_details = []
        topics_scanned = 0

        for topic in existing_topics:
            topics_scanned += 1
            similarity = self._calculate_topic_similarity(topic, new_topic)

            self._debug_print(
                f"  ğŸ¯ ä¸ '{topic['topic_name']}' çš„ç›¸ä¼¼åº¦: {similarity:.4f} "
                f"{'âœ… è¶…è¿‡é˜ˆå€¼' if similarity > self.similarity_threshold else 'âŒ æœªè¶…è¿‡'}",
                "SIMILARITY_SCAN")

            if similarity > self.similarity_threshold:
                similar_topic_ids.append({
                    'id': topic['topic_id'],
                    'name': topic['topic_name'],
                    'similarity': similarity
                })
                similarity_details.append({
                    'topic1': new_topic['topic_name'],
                    'topic2': topic['topic_name'],
                    'similarity': similarity
                })

        self._debug_print(f"ğŸ“Š æ‰«æå®Œæˆ: æ£€æŸ¥äº†{topics_scanned}ä¸ªè¯é¢˜, å‘ç°{len(similar_topic_ids)}ä¸ªç›¸ä¼¼è¯é¢˜",
                          "SIMILARITY_SCAN")

        # ç¬¬äºŒæ­¥ï¼šæ ¹æ®ç›¸ä¼¼è¯é¢˜æƒ…å†µè¿›è¡Œå¤„ç†
        if similar_topic_ids:
            self._debug_print(f"ğŸ¤ å‘ç°ç›¸ä¼¼è¯é¢˜ï¼Œå¼€å§‹åˆå¹¶å¤„ç†...", "MERGE")
            self._debug_print(f"  å‘ç° {len(similar_topic_ids)} ä¸ªç›¸ä¼¼è¯é¢˜:", "MERGE")
            for sim in similar_topic_ids:
                self._debug_print(f"    - {sim['name']} (ç›¸ä¼¼åº¦: {sim['similarity']:.4f})", "MERGE")

            # æ”¶é›†æ‰€æœ‰è¦åˆå¹¶çš„è¯é¢˜ID
            all_topic_ids = [new_topic['topic_id']]
            all_topic_names = [new_topic['topic_name']]

            for sim_topic in similar_topic_ids:
                all_topic_ids.append(sim_topic['id'])
                all_topic_names.append(sim_topic['name'])

            # æ‰§è¡Œåˆå¹¶
            return self._create_major_topic_from_topics(
                group_id=group_id,
                topic_ids=all_topic_ids,
                topic_names=all_topic_names,
                similarity_details=similarity_details
            )
        else:
            # æ²¡æœ‰ç›¸ä¼¼è¯é¢˜ï¼Œä½œä¸ºç‹¬ç«‹è¯é¢˜
            self._debug_print(f"ğŸ“Œ æ²¡æœ‰å‘ç°ç›¸ä¼¼è¯é¢˜ï¼Œä½œä¸ºç‹¬ç«‹è¯é¢˜", "TOPIC_ADD")

            # é‡å»ºå›¾ç»“æ„
            self._build_enhanced_graph()

            if self.json_file:
                self.save_to_json()

            self._debug_print(f"âœ… è¯é¢˜ '{topic_name}' å·²ä½œä¸ºç‹¬ç«‹è¯é¢˜æ·»åŠ ", "SUCCESS")
            self._debug_current_structure()

            return True, f"è¯é¢˜ '{topic_name}' å·²ä½œä¸ºç‹¬ç«‹è¯é¢˜æ·»åŠ "

    def _create_major_topic_from_topics(self, group_id: str, topic_ids: List[str],
                                        topic_names: List[str], similarity_details: List[Dict]) -> Tuple[bool, str]:
        """ä»å¤šä¸ªè¯é¢˜åˆ›å»ºå¤§è¯é¢˜ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        if len(topic_ids) < 2:
            self._debug_print(f"âŒ éœ€è¦è‡³å°‘2ä¸ªè¯é¢˜æ‰èƒ½åˆ›å»ºå¤§è¯é¢˜", "ERROR")
            return False, "éœ€è¦è‡³å°‘2ä¸ªè¯é¢˜æ‰èƒ½åˆ›å»ºå¤§è¯é¢˜"

        self._debug_print(f"ğŸ—ï¸ åˆ›å»ºæ–°çš„å¤§è¯é¢˜ï¼ŒåŒ…å«{len(topic_ids)}ä¸ªè¯é¢˜", "MERGE")

        # æŸ¥æ‰¾ç¾¤ç»„
        group = None
        for g in self.chat_groups:
            if g['group_id'] == group_id:
                group = g
                break

        if not group:
            self._debug_print(f"âŒ ç¾¤ç»„ {group_id} ä¸å­˜åœ¨", "ERROR")
            return False, f"ç¾¤ç»„ {group_id} ä¸å­˜åœ¨"

        # éªŒè¯æ‰€æœ‰è¯é¢˜éƒ½å­˜åœ¨
        valid_topic_ids = []
        valid_topic_names = []

        for topic_id, topic_name in zip(topic_ids, topic_names):
            topic = self.get_topic_details(topic_id)
            if topic:
                # æ£€æŸ¥è¯é¢˜çŠ¶æ€
                if topic.get('is_major'):
                    self._debug_print(f"  âš ï¸ è·³è¿‡å¤§è¯é¢˜: {topic_name}", "MERGE")
                    continue

                if topic.get('parent_id'):
                    self._debug_print(f"  âš ï¸ è·³è¿‡å·²æœ‰çˆ¶è¯é¢˜çš„è¯é¢˜: {topic_name}", "MERGE")
                    continue

                valid_topic_ids.append(topic_id)
                valid_topic_names.append(topic_name)
                self._debug_print(f"  âœ… å¯åˆå¹¶è¯é¢˜: {topic_name}", "MERGE")
            else:
                self._debug_print(f"  âš ï¸ æœªæ‰¾åˆ°è¯é¢˜: {topic_id} ({topic_name})", "MERGE")

        if len(valid_topic_ids) < 2:
            self._debug_print(f"âŒ æœ‰æ•ˆå¯åˆå¹¶è¯é¢˜ä¸è¶³2ä¸ª", "ERROR")
            return False, "æœ‰æ•ˆå¯åˆå¹¶è¯é¢˜ä¸è¶³2ä¸ª"

        # ç”Ÿæˆå¤§è¯é¢˜åç§°
        major_topic_name = self._generate_major_topic_name(valid_topic_names)
        self._debug_print(f"  ç”Ÿæˆçš„å¤§è¯é¢˜åç§°: {major_topic_name}", "MERGE")

        # åˆ›å»ºå¤§è¯é¢˜ID
        major_topic_id = f"major_{group_id.replace('group_', '')}_{uuid.uuid4().hex[:8]}"

        # åˆ›å»ºå¤§è¯é¢˜
        major_topic = {
            "topic_id": major_topic_id,
            "topic_name": major_topic_name,
            "priority": "ä¸­",
            "summaries": [f"åŒ…å«å­è¯é¢˜ï¼š{', '.join(valid_topic_names)}"],
            "related_records": [],
            "related_topics": [],
            "is_major": True,
            "parent_id": None,
            "child_count": len(valid_topic_ids)
        }

        # æ·»åŠ å¤§è¯é¢˜åˆ°ç¾¤ç»„
        group['topics'].append(major_topic)
        self._debug_print(f"  å¤§è¯é¢˜å·²æ·»åŠ åˆ°ç¾¤ç»„", "MERGE")

        # æ›´æ–°å­è¯é¢˜çš„çˆ¶ID
        success_count = 0
        for topic_id in valid_topic_ids:
            success = self._update_topic_parent(topic_id, major_topic_id)
            if success:
                success_count += 1
                topic_name = self.get_topic_name_by_id(topic_id)
                self._debug_print(f"  æ›´æ–°å­è¯é¢˜çˆ¶ID: {topic_name} -> {major_topic_name}", "MERGE")
            else:
                self._debug_print(f"  âŒ æ›´æ–°å­è¯é¢˜çˆ¶IDå¤±è´¥: {topic_id}", "MERGE")

        # é‡å»ºå›¾ç»“æ„
        self._build_enhanced_graph()

        if self.json_file:
            self.save_to_json()

        self._debug_print(f"âœ… æˆåŠŸåˆ›å»ºå¤§è¯é¢˜ '{major_topic_name}'ï¼ŒåŒ…å« {success_count} ä¸ªå­è¯é¢˜", "SUCCESS")
        self._debug_current_structure()

        return True, f"âœ… æˆåŠŸåˆ›å»ºå¤§è¯é¢˜ '{major_topic_name}'ï¼ŒåŒ…å« {success_count} ä¸ªç›¸ä¼¼è¯é¢˜"

    def _update_topic_parent(self, topic_id: str, parent_id: str) -> bool:
        """æ›´æ–°è¯é¢˜çš„çˆ¶è¯é¢˜"""
        for group in self.chat_groups:
            for topic in group.get('topics', []):
                if topic['topic_id'] == topic_id:
                    topic['parent_id'] = parent_id
                    return True
        return False

    def _calculate_topic_similarity(self, topic1: Dict, topic2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¯é¢˜çš„ç»¼åˆç›¸ä¼¼åº¦"""
        self._debug_print(f"ğŸ” è®¡ç®—è¯é¢˜ç›¸ä¼¼åº¦:", "SIMILARITY")
        self._debug_print(f"  è¯é¢˜A: {topic1.get('topic_name', '')}", "SIMILARITY")
        self._debug_print(f"  è¯é¢˜B: {topic2.get('topic_name', '')}", "SIMILARITY")

        # 1. è¯é¢˜åç§°ç›¸ä¼¼åº¦
        name_sim = self._calculate_text_similarity(
            topic1.get('topic_name', ''),
            topic2.get('topic_name', '')
        )

        # 2. å­—ç¬¦ä¸²å‰ç¼€ç›¸ä¼¼åº¦
        prefix_sim = self._calculate_prefix_similarity(
            topic1.get('topic_name', ''),
            topic2.get('topic_name', '')
        )

        # 3. æ‘˜è¦ç›¸ä¼¼åº¦
        summary1 = ' '.join(topic1.get('summaries', []))
        summary2 = ' '.join(topic2.get('summaries', []))
        summary_sim = self._calculate_text_similarity(summary1, summary2)

        # 4. è¯é¢˜ç›¸ä¼¼åº¦å’Œæ‘˜è¦ç›¸ä¼¼åº¦å–æœ€å¤§å€¼
        topic_similarity = name_sim
        max_topic_summary_sim = max(topic_similarity, summary_sim)

        # ç»¼åˆè®¡ç®—
        total_similarity = (max_topic_summary_sim * 0.4) + (prefix_sim * 0.6)

        self._debug_print(f"  åç§°ç›¸ä¼¼åº¦: {name_sim:.4f}", "SIMILARITY")
        self._debug_print(f"  å‰ç¼€ç›¸ä¼¼åº¦: {prefix_sim:.4f}", "SIMILARITY")
        self._debug_print(f"  æ‘˜è¦ç›¸ä¼¼åº¦: {summary_sim:.4f}", "SIMILARITY")
        self._debug_print(f"  è¯é¢˜ç›¸ä¼¼åº¦: {topic_similarity:.4f}", "SIMILARITY")
        self._debug_print(f"  max(è¯é¢˜,æ‘˜è¦): {max_topic_summary_sim:.4f}", "SIMILARITY")
        self._debug_print(f"  ç»¼åˆç›¸ä¼¼åº¦: {total_similarity:.4f}", "SIMILARITY")
        self._debug_print(
            f"  é˜ˆå€¼({self.similarity_threshold}): {'âœ… è¶…è¿‡' if total_similarity > self.similarity_threshold else 'âŒ æœªè¶…è¿‡'}",
            "SIMILARITY")

        # è®°å½•ç›¸ä¼¼åº¦è®¡ç®—
        self.similarity_calculations.append({
            'topic1': topic1.get('topic_name', ''),
            'topic2': topic2.get('topic_name', ''),
            'name_sim': name_sim,
            'prefix_sim': prefix_sim,
            'summary_sim': summary_sim,
            'total_similarity': total_similarity,
            'timestamp': datetime.now().isoformat()
        })

        return total_similarity

    def _calculate_prefix_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²å‰ç¼€ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        min_len = min(len(text1), len(text2))
        if min_len == 0:
            return 0.0
        common = 0
        for i in range(min_len):
            if text1[i] == text2[i]:
                common += 1
            else:
                break
        return common / max(len(text1), len(text2))

    def _jieba_tokenizer(self, text: str) -> List[str]:
        """ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯"""
        if not text or not text.strip():
            return []
        words = jieba.lcut(text.strip(), cut_all=False)
        filtered_words = [w for w in words if w.strip() and len(w.strip()) > 1]
        return filtered_words

    def _fit_vectorizer_vocab(self):
        """ä½¿ç”¨ä¸€äº›åˆå§‹è¯é¢˜åç§°æ‹ŸåˆTF-IDFå‘é‡åŒ–å™¨çš„è¯æ±‡è¡¨"""
        initial_texts = [
            "æ¯”èµ›ç»å†", "æ¯”èµ›å¥–åŠ±", "å­¦ä¹ è®¨è®º", "å·¥ä½œäº¤æµ",
            "æŠ€æœ¯åˆ†äº«", "é¡¹ç›®ç»éªŒ", "é—®é¢˜è§£ç­”", "æ—¥å¸¸èŠå¤©"
        ]
        try:
            self.vectorizer.fit_transform(initial_texts)
        except Exception as e:
            self._debug_print(f"âš ï¸ TF-IDFå‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}", "WARNING")

    def load_from_json(self, json_file: str) -> bool:
        """ä»JSONæ–‡ä»¶åŠ è½½"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.chat_groups = data.get('chat_groups', [])
            self.json_file = json_file

            # æ„å»ºå›¾ç»“æ„
            self._build_enhanced_graph()

            self._debug_print(f"âœ… æˆåŠŸä» {json_file} åŠ è½½ {len(self.chat_groups)} ä¸ªç¾¤èŠ", "INFO")
            return True
        except FileNotFoundError:
            self._debug_print(f"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {json_file}", "WARNING")
            return False
        except json.JSONDecodeError as e:
            self._debug_print(f"âš ï¸ JSONè§£æé”™è¯¯: {e}", "ERROR")
            return False
        except Exception as e:
            self._debug_print(f"âš ï¸ åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}", "ERROR")
            return False

    def get_topic_details(self, topic_id: str) -> Optional[Dict]:
        """è·å–è¯é¢˜è¯¦ç»†ä¿¡æ¯"""
        if not topic_id:
            return None

        for group in self.chat_groups:
            for topic in group.get('topics', []):
                if topic['topic_id'] == topic_id:
                    return topic
        return None

    def get_topic_name_by_id(self, topic_id: str) -> str:
        """æ ¹æ®è¯é¢˜IDè·å–è¯é¢˜åç§°"""
        if not topic_id:
            return ""

        topic = self.get_topic_details(topic_id)
        return topic['topic_name'] if topic else ""

    def save_to_json(self, json_file: str = None) -> bool:
        """ä¿å­˜åˆ°JSONæ–‡ä»¶"""
        if json_file is None:
            json_file = self.json_file

        if not json_file:
            self._debug_print("âš ï¸ æœªæŒ‡å®šä¿å­˜æ–‡ä»¶è·¯å¾„", "WARNING")
            return False

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(json_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)

        data = {
            'chat_groups': self.chat_groups,
            'metadata': {
                'similarity_threshold': self.similarity_threshold,
                'enable_api': self.enable_api,
                'generated_at': datetime.now().isoformat(),
                'debug_logs': self.debug_logs[-100:] if self.debug_mode else [],
                'similarity_calculations': self.similarity_calculations[-50:] if self.debug_mode else []
            }
        }

        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self._debug_print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {json_file}", "INFO")
            return True
        except Exception as e:
            self._debug_print(f"âŒ ä¿å­˜å¤±è´¥: {e}", "ERROR")
            return False

    def _generate_major_topic_name(self, subtopic_names: List[str]) -> str:
        """ç”Ÿæˆå¤§è¯é¢˜åç§°"""
        if not subtopic_names:
            return "ç»¼åˆè®¨è®º"

        if not self.enable_api or not self.client:
            if len(subtopic_names) == 1:
                return f"å…³äº{subtopic_names[0]}çš„è®¨è®º"
            else:
                return f"ç»¼åˆè®¨è®ºï¼š{subtopic_names[0]}ç­‰"

        try:
            response = self.client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[
                    {
                        'role': 'system',
                        'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è¯é¢˜åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„å­è¯é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„å¤§è¯é¢˜åç§°ã€‚'
                    },
                    {
                        'role': 'user',
                        'content': f"è¯·ä¸ºä»¥ä¸‹å­è¯é¢˜ç”Ÿæˆä¸€ä¸ªåˆé€‚çš„å¤§è¯é¢˜åç§°ï¼š{', '.join(subtopic_names)}ã€‚è¦æ±‚ï¼š1. ç®€æ´æ˜äº† 2. æ¶µç›–æ‰€æœ‰å­è¯é¢˜ 3. ä¸è¶…è¿‡15ä¸ªå­—"
                    }
                ],
                max_tokens=50,
                temperature=0.7
            )

            major_topic_name = response.choices[0].message.content.strip()
            major_topic_name = major_topic_name.strip('"\'')
            return major_topic_name
        except Exception as e:
            self._debug_print(f"âš ï¸ APIè°ƒç”¨å¤±è´¥: {e}", "WARNING")
            return f"ç»¼åˆè¯é¢˜ï¼š{subtopic_names[0]}ç­‰"

    def get_topic_hierarchy(self, group_id: str = None) -> Dict:
        """è·å–è¯é¢˜å±‚çº§ç»“æ„"""
        hierarchy = {
            'major_topics': [],
            'orphan_topics': [],
            'statistics': {
                'total_major': 0,
                'total_children': 0,
                'total_orphan': 0
            }
        }

        for group in self.chat_groups:
            if group_id and group['group_id'] != group_id:
                continue

            for topic in group.get('topics', []):
                if topic.get('is_major'):
                    children = []
                    child_ids = self.parent_child_map.get(topic['topic_id'], [])
                    for child_id in child_ids:
                        child = self.get_topic_details(child_id)
                        if child:
                            children.append({
                                'id': child['topic_id'],
                                'name': child['topic_name'],
                                'priority': child['priority']
                            })

                    hierarchy['major_topics'].append({
                        'id': topic['topic_id'],
                        'name': topic['topic_name'],
                        'group_id': group['group_id'],
                        'group_name': group['group_name'],
                        'children': children,
                        'child_count': len(children)
                    })
                    hierarchy['statistics']['total_major'] += 1
                    hierarchy['statistics']['total_children'] += len(children)
                elif not topic.get('parent_id'):
                    hierarchy['orphan_topics'].append({
                        'id': topic['topic_id'],
                        'name': topic['topic_name'],
                        'group_id': group['group_id'],
                        'group_name': group['group_name'],
                        'priority': topic['priority']
                    })
                    hierarchy['statistics']['total_orphan'] += 1

        return hierarchy

    def _build_enhanced_graph(self):
        """æ„å»ºå¢å¼ºçš„å›¾ç»“æ„"""
        self.graph.clear()
        self.topic_id_to_name.clear()
        self.topic_name_to_id.clear()
        self.parent_child_map.clear()
        self.child_parent_map.clear()
        self.topic_id_to_type.clear()
        self.topic_embeddings.clear()

        # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰è¯é¢˜
        for group in self.chat_groups:
            group_id = group['group_id']
            for topic in group.get('topics', []):
                topic_id = topic['topic_id']
                topic_name = topic['topic_name']

                self.topic_id_to_name[topic_id] = topic_name
                self.topic_name_to_id[topic_name] = topic_id

                self.graph[topic_id] = {
                    'id': topic_id,
                    'name': topic_name,
                    'group_id': group_id,
                    'is_major': topic.get('is_major', False),
                    'parent_id': topic.get('parent_id'),
                    'children': [],
                    'related': []
                }

        # ç¬¬äºŒéï¼šæ„å»ºç±»å‹æ˜ å°„å’Œå…³ç³»
        for group in self.chat_groups:
            for topic in group.get('topics', []):
                topic_id = topic['topic_id']

                if topic.get('is_major'):
                    self.topic_id_to_type[topic_id] = 'major'
                elif topic.get('parent_id'):
                    self.topic_id_to_type[topic_id] = 'child'
                else:
                    self.topic_id_to_type[topic_id] = 'orphan'

                parent_id = topic.get('parent_id')
                if parent_id and parent_id in self.graph:
                    self.child_parent_map[topic_id] = parent_id
                    if parent_id not in self.parent_child_map:
                        self.parent_child_map[parent_id] = []
                    self.parent_child_map[parent_id].append(topic_id)

                    if parent_id in self.graph:
                        self.graph[parent_id]['children'].append(topic_id)

                for related_topic_name in topic.get('related_topics', []):
                    related_topic_id = self.topic_name_to_id.get(related_topic_name)
                    if related_topic_id and related_topic_id != topic_id:
                        if related_topic_id not in self.graph[topic_id]['related']:
                            self.graph[topic_id]['related'].append(related_topic_id)

        self._debug_print(
            f"ğŸ“Š å›¾ç»“æ„æ„å»ºå®Œæˆ: {len(self.graph)}ä¸ªèŠ‚ç‚¹, {sum(len(v['children']) for v in self.graph.values())}æ¡çˆ¶å­å…³ç³»",
            "GRAPH")

    def _debug_current_structure(self):
        """è¾“å‡ºå½“å‰è¯é¢˜ç»“æ„"""
        hierarchy = self.get_topic_hierarchy()

        self._debug_print("ğŸ“Š å½“å‰è¯é¢˜ç»“æ„:", "STRUCTURE")
        self._debug_print(f"  å¤§è¯é¢˜æ•°: {len(hierarchy.get('major_topics', []))}", "STRUCTURE")
        self._debug_print(f"  å­è¯é¢˜æ•°: {hierarchy.get('statistics', {}).get('total_children', 0)}", "STRUCTURE")
        self._debug_print(f"  ç‹¬ç«‹è¯é¢˜: {hierarchy.get('statistics', {}).get('total_orphan', 0)}", "STRUCTURE")

        for i, major in enumerate(hierarchy.get('major_topics', []), 1):
            self._debug_print(f"  {i}. ğŸ¢ {major['name']} ({major['child_count']}ä¸ªå­è¯é¢˜)", "STRUCTURE")
            for j, child in enumerate(major.get('children', []), 1):
                self._debug_print(f"     {j}. ğŸ”— {child['name']}", "STRUCTURE")

        for i, orphan in enumerate(hierarchy.get('orphan_topics', []), 1):
            self._debug_print(f"  {i}. ğŸ”¸ {orphan['name']} (ç‹¬ç«‹è¯é¢˜)", "STRUCTURE")

    def test_similarity_and_merge(self, test_data: List[Tuple[str, str, str]] = None):
        """
        æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—å’Œåˆå¹¶åŠŸèƒ½

        å…³é”®ä¿®å¤ï¼šç¡®ä¿æµ‹è¯•æ•°æ®èƒ½è§¦å‘åˆå¹¶
        """
        if test_data is None:
            test_data = [
                ("æ”¯æ•™é¡¹ç›®æ‹›å‹Ÿ", "æ‹›å‹Ÿæ”¯æ•™é¡¹ç›®çš„å¿—æ„¿è€…", "é«˜"),
                ("éé—é¡¹ç›®æ‹›å‹Ÿ", "æ‹›å‹Ÿéé—é¡¹ç›®çš„å‚ä¸è€…", "é«˜"),
                ("2025å¹´æŒ‘æˆ˜æ¯å›¢é˜Ÿæ‹›å‹Ÿéé—å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿæˆå‘˜",
                 "2025å¹´æŒ‘æˆ˜æ¯é¡¹ç›®ï¼Œæ‹›å‹Ÿéé—å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿå¼€å‘æˆå‘˜", "é«˜"),
                ("2025å¹´æŒ‘æˆ˜æ¯å›¢é˜Ÿæ‹›å‹Ÿç½‘é¡µè®¾è®¡å’Œè´¢åŠ¡ç®¡ç†äººå‘˜",
                 "2025å¹´æŒ‘æˆ˜æ¯é¡¹ç›®ï¼Œæ‹›å‹Ÿç½‘é¡µè®¾è®¡å¸ˆå’Œè´¢åŠ¡ç®¡ç†äººå‘˜", "é«˜"),
            ]

        self._debug_print("ğŸ§ª å¼€å§‹ç›¸ä¼¼åº¦è®¡ç®—å’Œåˆå¹¶æµ‹è¯•", "TEST")
        self._debug_print("=" * 60, "TEST")

        for i, (topic_name, description, priority) in enumerate(test_data, 1):
            self._debug_print(f"æµ‹è¯• {i}/{len(test_data)}: æ·»åŠ è¯é¢˜ '{topic_name}'", "TEST")

            success, message = self.add_topic_simple(
                group_id="group_test_001",
                topic_name=topic_name,
                priority=priority,
                description=description
            )

            if success:
                self._debug_print(f"âœ… {message}", "TEST")
            else:
                self._debug_print(f"âŒ {message}", "TEST")

            self._debug_print("-" * 40, "TEST")

        self._debug_print("=" * 60, "TEST")
        self._debug_print("ğŸ§ª æµ‹è¯•å®Œæˆ", "TEST")

        # æ˜¾ç¤ºæœ€ç»ˆç»“æ„
        hierarchy = self.get_topic_hierarchy()
        self._debug_print(f"æœ€ç»ˆç»“æ„ç»Ÿè®¡:", "TEST")
        self._debug_print(f"  å¤§è¯é¢˜æ•°: {len(hierarchy.get('major_topics', []))}", "TEST")
        self._debug_print(f"  å­è¯é¢˜æ•°: {hierarchy.get('statistics', {}).get('total_children', 0)}", "TEST")
        self._debug_print(f"  ç‹¬ç«‹è¯é¢˜: {hierarchy.get('statistics', {}).get('total_orphan', 0)}", "TEST")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•TopicGraphåˆå¹¶åŠŸèƒ½...")

    # ç¡®ä¿outputç›®å½•å­˜åœ¨
    os.makedirs("output", exist_ok=True)

    # åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
    test_data = {
        "chat_groups": [
            {
                "group_id": "group_test_001",
                "group_name": "æµ‹è¯•ç¾¤èŠ",
                "description": "ç”¨äºæµ‹è¯•çš„è¯é¢˜ç¾¤ç»„",
                "topics": []
            }
        ]
    }

    json_file_path = "output/topic_graph_data.json"

    with open(json_file_path, "w", encoding='utf-8') as f:
        json.dump(test_data, f, indent=2)

    print(f"âœ… åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {json_file_path}")

    # åˆ›å»ºå¹¶æµ‹è¯•TopicGraph
    topic_graph = TopicGraph(
        json_file=json_file_path,
        similarity_threshold=0.1,  # ä½¿ç”¨è¾ƒä½çš„é˜ˆå€¼ç¡®ä¿åˆå¹¶
        debug_mode=True
    )

    # è¿è¡Œæµ‹è¯•
    topic_graph.test_similarity_and_merge()

    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼æŸ¥çœ‹ä¸Šé¢çš„è¾“å‡ºäº†è§£åˆå¹¶è¿‡ç¨‹ã€‚")
    print("=" * 60)